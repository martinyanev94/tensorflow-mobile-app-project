dependencies {
    implementation 'org.tensorflow:tensorflow-lite:2.4.0'
    implementation 'org.tensorflow:tensorflow-lite-gpu:2.4.0'
}
import org.tensorflow.lite.Interpreter;

public class TensorFlowLiteModel {
    private Interpreter interpreter;

    public TensorFlowLiteModel(String modelPath) {
        // Load the TFLite model
        interpreter = new Interpreter(loadModelFile(modelPath));
    }

    private MappedByteBuffer loadModelFile(String modelPath) {
        // Load model file from the assets folder
        AssetFileDescriptor fileDescriptor = context.getAssets().openFd(modelPath);
        FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());
        FileChannel fileChannel = inputStream.getChannel();
        long startOffset = fileDescriptor.getStartOffset();
        long declaredLength = fileDescriptor.getDeclaredLength();
        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);
    }

    public float[][] runInference(float[][] input) {
        float[][] output = new float[1][NUM_CLASSES]; // Adjust according to your number of classes
        interpreter.run(input, output);
        return output;
    }
}
